mean(abs(pvals1 - pvals2) < 1e-5 )
dim(pvalueData)
dim(pvalueData3)
rm(list=ls())
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueDataTest.rda")
dim(pvalueData)
rm(list=ls())
pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueDataTest.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueDataTest.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueData.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueData.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}
tmpData
i
j
tmpData = getAbstractsPmids(journals[i],years[j])
while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}
barplot(c(1,1,-3))
?barplot
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12[1,]
pres08 = read.csv("~/Desktop/2008pres.csv")
pres08[1,]
sum(pres12$FIPS == 0)
table(pres12$X[pres12$FIPS==0,])
table(pres12$X[pres12$FIPS==0])
pres12[pres12$X=="OK",]
pres12 = pres12[pres12$FIPS==0,]
dim(pres12)
pres12[1,]
pres12[2,]
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12[1:10,]
pres12[21:30,]
pres12[50:100,]
pres12 = pres12[pres12$FIPS==0,]
length(unique(pres12$X))
which.max(pres12$X == "OK")
which(pres12$X == "OK")
pres12[c(36,37)]
pres12[c(36,37),]
pres12[1,]
pres12[1,5]
pres12[1,4]
as.numeric(pres12[1,4])
as.numeric(as.character(pres12[1,4])
)
as.numeric(as.character(pres12[1,4]))
pres08[1,]
sum(pres08$LAST.NAME == "McCain")
sum(pres08$LAST.NAME == "McCain" & pres08$STATE=="Alabama")
pres08[pres08$LAST.NAME == "McCain",]
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12 = pres12[pres12$FIPS==0,]
pres12[1,]
table(unique(pres12))
table(unique(pres12$x))
table(unique(pres12$X))
substr
gsub
gsub(pres12[1,3])
pres12[1,]
pres12$Obama.vote[1]
dem12 = as.character(pres12$Obama.vote)
dem12[1]
dem12 = sapply(dem12,function(x){gsub(x,",","")})
dem12[1]
dem12 = as.character(pres12$Obama.vote)
gsub(dem12[1],",","")
?gsub
gsub(",","",dem12[1])
dem12 = as.character(pres12$Obama.vote)
dem12 = as.numeric(sapply(dem12,function(x){gsub(",","",x)}))
dem12[1]
hist(dem12)
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12 = pres12[pres12$FIPS==0,]
dem12 = as.character(pres12$Obama.vote)
dem12 = as.numeric(sapply(dem12,function(x){gsub(",","",x)}))
rep12 = as.character(pres12$Romney.vote)
rep12 = as.numeric(sapply(rep12,function(x){gsub(",","",x)}))
plot(rep12,dem12)
rep12[1]
pres12[1,]
table(pres12[,2])
res12 = read.csv("~/Desktop/2012-pres.csv")
pres12 = pres12[pres12$FIPS==0,]
table(pres12[,2])
table(pres12[,1])
library(XML)
theurl <- "http://uselectionatlas.org/RESULTS/data.php?year=2008&datatype=national&def=1&f=0&off=0&elect=0"
tables <- readHTMLTable(theurl)
tables[1,]
dim(tables)
tables[1]
tables
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
n.rows
library(slidify)
?author
setwd("~/Documents/Work/teaching/2013/coursera/")
ls()
list.files()
author("gettingStarted")
slidify("gettingStarted")
ls()
list.files()
slidify("index.Rmd")
setwd("~/Documents/Work/teaching/2013/coursera/")
setwd("gettingHelp/")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
getwd()
setwd("../aboutDataAnalysis/")
slidify("index.Rmd")
setwd("~/Dropbox/Jeff/teaching/2013/")
setwd("753/lectures/")
setwd("001courseMotivation/")
library(slidify)
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
setwd("~/Dropbox/Jeff/teaching/2013/753/jhsph753/lectures/001courseMotivation/")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
browseURL("index.html")
setwd("~/Dropbox/Jeff/teaching/2013/753/jhsph753/intros/003intro/")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
ovals <- read.table("~/Downloads/RD_501_42101_2011[1]/RD_501_42101_2011-0.txt",sep="|",nlines=100)
ovals <- read.table("~/Downloads/RD_501_42101_2011[1]/RD_501_42101_2011-0.txt",sep="|",n=100)
args(read.table)
ovals <- read.table("~/Downloads/RD_501_42101_2011[1]/RD_501_42101_2011-0.txt",sep="|",nrows=100)
ovals[1,]
ovals <- read.table("~/Downloads/RD_501_42101_2011[1]/RD_501_42101_2011-0.txt",sep="|",nrows=100,header=T)
ovals[1,]
ovals
?read.table
dim(ovals)
ovals[1,2]
ovals[1,3]
ovals[1,4]
ovals <- read.table("~/Downloads/RD_501_88101_2012[1]/RD_501_88101_2012-0.txt",sep="|",nrows=100,header=T)
ovals[1,]
ovals[2,]
ovals <- read.table("~/Downloads/RD_501_88101_2012[1]/RD_501_88101_2012-0.txt",sep="|",nrows=100)
ovals[1,]
class(ovals)
ovals <- read.table("~/Downloads/RD_501_88101_2012[1]/RD_501_88101_2012-0.txt",sep="|")
dim(ovals)
ovals[1,]
ovals <- read.table("~/Downloads/RD_501_88101_2012[1]/RD_501_88101_2012-0.txt,sep="|")
_
ovals <- read.table("~/Downloads/RD_501_88101_2012[1]/RD_501_88101_2012-0.txt",sep="|")
oval[1,]
sum(is.na(ovals))
dim(ovals)
library(sandwich)
?vcovHC
vcovHC
x
y
x <- rnorm(10)
y <- rnorm(10)
lm1 <- lm(y ~ x)
summary(lm1)
vcovHC(lm1)
X <- rbind(rep(1,10),x)
x <- rnorm(10)
z <- rnorm(10)
y <- rnorm(10)
x <- (y > z)
lm(y ~ x + z)
plot(lm(y ~ x + z)$residuals)
plot(lm(y ~ x + z)$residuals,z)
plot(lm(y ~ x + z)$residuals,x)
x1 <- rnorm(10)
x2 <- rnorm(10)
x1[11] <- 15
x1
x2
t.test(x1,x2)
x1 <- rnorm(10)
x1 <- c(x1,15,25)
x2 <- rnorm(10)
t.test(x1,x2)
x1 <- c(x1,15,50)
t.test(x1,x2)
x1 <- c(x1,15,1000)
t.test(x1,x2)
x1 <- c(x1,50,50)
t.test(x1,x2)
source("http://www.bioconductor.org/biocLite.R")
biocLite("breastCancerNKI")
astorRepos <- "http://astor.som.jhmi.edu/~marchion/software/"
install.packages("seventyGeneData", repos = astorRepos)
library(seventyGeneData)
astorRepos <- "http://astor.som.jhmi.edu/~marchion/software/"
astorRepos <- "http://astor.som.jhmi.edu/~marchion/software/"
install.packages("seventyGeneData", repos = astorRepos)
astorRepos <- "http://astor.som.jhmi.edu/~marchion/software/"
install.packages("mammaPrintData", repos = astorRepos)
astorRepos <- "http://astor.som.jhmi.edu/~marchion/software/"
install.packages("KTSP", repos = astorRepos)
astorRepos <- "http://astor.som.jhmi.edu/~marchion/software/"
install.packages("seventyGeneData", repos = astorRepos)
library(XML)
htmlRaven=htmlTreeParse("http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens",useInternalNodes=TRUE)
## Choose the list items with class='score' ##
Score=xpathSApply(htmlRaven,"//li[@class='score']",xmlValue)
## Choose the list items with class='team-name' ##
TeamName=xpathSApply(htmlRaven,"//li[@class='team-name']",xmlValue)
## Choose the whole unordered list with class='score' ##
WinLoss=xpathSApply(htmlRaven,"//ul[@class='game-schedule']//span",xmlValue)
## This is tricky ##
## First get all the //td items under the rows(//tr) whose @class contains 'row' ##
## Choose those which contains ',', which are the actual dates ##
## There is no class or id under the date component. I had to do this. ##
Date=grep(',',xpathSApply(htmlRaven,"//tr[contains(@class,'row')]//td",xmlValue),value=TRUE)
## Combine the data ##
RavenData=data.frame(date=Date,team=TeamName,status=WinLoss,score=Score)
RavenData[1,]
RavenDAta
RavenDat
RavenData
args(strsplit)
firstScore <- sapply(RavenData$score,function(x){strsplit(x,"-")[[1]][1]}
)
strsplit(RavenData$score[1],"-")
class(RavenData$score)
firstScore <- sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]][1]})
firstScore
firstScore <- sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]]})
firstScore
strsplit(RavenData$score[1],"-")
strsplit(as.character(RavenData$score[1]),"-")
strsplit(as.character(RavenData$score[1]),"-")[[1]][2]
secondScore <- sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]][2]})
secondScore
secondScore <- sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]][2]})
secondScore <- sapply(secondScore,function(x){strsplit(x," ")[[1]][1]})
secondScore
ravenData
RavenData
?ifelse
ravenScore <- ifelse(RavenData$status=="W",firstScore,secondScore)
ravenScore
## Hack the scores a bit
firstScore <- as.numeric(sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]][1]}))
secondScore <- sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]][2]})
secondScore <- sapply(secondScore,function(x){strsplit(x," ")[[1]][1]})
ravenScore <- ifelse(RavenData$status=="W",firstScore,secondScore)
ravenScore
opponentScore <- ifelse(RavenData$status=="L",firstScore,secondScore)
opponentScore
## Hack the scores a bit
firstScore <- as.numeric(sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]][1]}))
secondScore <- sapply(as.character(RavenData$score),function(x){strsplit(x,"-")[[1]][2]})
secondScore <- sapply(secondScore,function(x){strsplit(x," ")[[1]][1]})
ravenScore <- as.numeric(ifelse(RavenData$status=="W",firstScore,secondScore))
opponentScore <- as.numeric(ifelse(RavenData$status=="L",firstScore,secondScore))
ravenScore
opponentScore
dim(RavenDAta)
dim(RavenData)
X <- model.matrix(~ RavenData$status=="W")
X
X <- model.matrix(~ RavenData$status=="W")
y <- ravenScore
betaHat <- solve(t(X)%*%X)%*%t(X)y
X <- model.matrix(~ RavenData$status=="W")
y <- ravenScore
betaHat <- solve(t(X)%*%X)%*%t(X)%*%y
betaHat
lm(ravenScore ~ RavenData$status=="W")
y - betaHat %*%X
dim(betaHat)
dim(X)
D <- diag(y - X %*% betaHat)
D
y - X %*% betaHat
D <- diag(as.vector(y - X %*% betaHat))
D
D <- diag(as.vector((y - X %*% betaHat)^2))
solve(t(X)%*%X) %*% t(X)%*%D %*% X %*% solve(t(X) %*% X)
D <- diag(as.vector((y - X %*% betaHat)^2))
Shat <- (1/20)*solve(t(X)%*%X) %*% t(X)%*%D %*% X %*% solve(t(X) %*% X)
Shat
library(sandwich)
?vcovHC
lm1 <- lm(ravenScore ~ RavenData$status=="W")
vcovHC(lm1)
vcovHC(lm1,type="HC0")
D <- diag(as.vector((y - X %*% betaHat)^2))
Shat <- (1/20)*solve(t(X)%*%X) %*% t(X)%*%D %*% X %*% solve(t(X) %*% X)''
D <- diag(as.vector((y - X %*% betaHat)^2))
Shat <- (1/20)*solve(t(X)%*%X) %*% t(X)%*%D %*% X %*% solve(t(X) %*% X)
Shat
D <- diag(as.vector((y - X %*% betaHat)^2))
Shat <- solve(t(X)%*%X) %*% t(X)%*%D %*% X %*% solve(t(X) %*% X)
Shat
D <- (y - X %*% betaHat) %*% (y - X %*% betaHat)^T
D <- (y - X %*% betaHat) %*% t(y - X %*% betaHat)
D
D <- (y - X %*% betaHat) %*% t(y - X %*% betaHat)
sHat0 <- solve(t(X)%*%X) %*% t(X)%*%D %*% X %*% solve(t(X) %*% X)
sHat0
lm2 <- lm(ravenScore ~ RavenData$status=="W")
lm2 <- lm(ravenScore ~ RavenData$status=="W" + opponentScore)
lm2 <- lm(ravenScore ~ (RavenData$status=="W") + opponentScore)
lm2
summary(lm2)
plot(lm2$residuals)
plot(lm2$residuals,opponentScore)
plot(lm2$residuals,ravenScore)
plot(lm2$residuals,lm2$fitted)
plot(lm2$residuals,ravenScore)
vcovHC(lm1,method="HC0")
?vcovHC
setwd("~/Dropbox/Jeff/teaching/2013/753/jhsph753/intros/007intro/")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
browseURL("index.html")
getwd()
browseURL("index.html")
getwd()
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
dataMatrix <- matrix(rnorm(100*10),nrow=10)
nExamples <- dim(dataMatrix)[1]
nSamples <- dim(dataMatrix)[2]
for(i in 1:dim(dataMatrix)[2]){
jj <- jitter(rep(1,nSamples))
plot(jj,dataMatrix[,i],type="n",xlim=c(0.5,1.5),xaxt="n")
}
nSamples <- dim(dataMatrix)[1]
nExamples <- dim(dataMatrix)[2]
for(i in 1:dim(dataMatrix)[2]){
jj <- jitter(rep(1,nSamples))
plot(jj,dataMatrix[,i],type="n",xlim=c(0.5,1.5),xaxt="n")
}
nSamples <- dim(dataMatrix)[1]
nExamples <- dim(dataMatrix)[2]
i
i = 1
jj <- jitter(rep(1,nSamples))
plot(jj,dataMatrix[,i],type="n",xlim=c(0.5,1.5),xaxt="n")
?text
jj <- jitter(rep(1,nSamples))
plot(jj,dataMatrix[,i],type="n",xlim=c(0.5,1.5),xaxt="n")
text(jj,dataMatrix[,i],labels=1:nSamples)
nSamples <- dim(dataMatrix)[1]
nExamples <- dim(dataMatrix)[2]
for(i in 1:dim(dataMatrix)[2]){
jj <- jitter(rep(1,nSamples))
plot(jj,dataMatrix[,i],type="n",xlim=c(0.5,1.5),xaxt="n")
text(jj,dataMatrix[,i],labels=1:nSamples,cex=cx)
}
cx = 0.5
nSamples <- dim(dataMatrix)[1]
nExamples <- dim(dataMatrix)[2]
for(i in 1:dim(dataMatrix)[2]){
jj <- jitter(rep(1,nSamples))
plot(jj,dataMatrix[,i],type="n",xlim=c(0.5,1.5),xaxt="n")
text(jj,dataMatrix[,i],labels=1:nSamples,cex=cx)
}
?readLine
?readLines
?readLine
/readline
?readline
